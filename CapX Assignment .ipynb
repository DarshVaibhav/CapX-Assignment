{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e726941",
   "metadata": {},
   "source": [
    "### Step 1: Data Scraping (Using Reddit with PRAW)\n",
    "\n",
    "1) Scrape dataset across multiple subreddits for diverse opinions.\n",
    "\n",
    "2) Include both post titles and comments to capture detailed sentiments.\n",
    "\n",
    "3) Use additional fields like post score and comment count for better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d83796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to 'reddit_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"fofmJW3tmiSRJt7OqNIZ9A\",\n",
    "    client_secret=\"fwFHA7NUS-gUmwfy-fSdqDI9XpAb6A\",\n",
    "    user_agent=\"StockSentimentApp/1.0 by u/Fun-Teach-2904\"\n",
    ")\n",
    "\n",
    "# Function to scrape Reddit posts\n",
    "def scrape_reddit(subreddit_name, limit=100):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "\n",
    "    for post in subreddit.hot(limit=limit):\n",
    "        posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"body\": post.selftext,\n",
    "            \"score\": post.score,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"created_utc\": post.created_utc\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "# Scrape data from r/stocks\n",
    "data = scrape_reddit(\"stocks\", limit=200)\n",
    "data.to_csv(\"reddit_data.csv\", index=False)\n",
    "print(\"Data scraped and saved to 'reddit_data.csv'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d441a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91955\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91955\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'cleaned_reddit_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\W\", \" \", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data[\"cleaned_title\"] = data[\"title\"].apply(clean_text)\n",
    "data[\"cleaned_body\"] = data[\"body\"].apply(clean_text)\n",
    "data.to_csv(\"cleaned_reddit_data.csv\", index=False)\n",
    "print(\"Cleaned data saved to 'cleaned_reddit_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3083d",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing and Sentiment Analysis\n",
    "\n",
    "1) Use VADER Sentiment Analysis for polarity scores (positive, negative, neutral).\n",
    "\n",
    "2) Include TF-IDF vectors for text features to capture context and relevance.\n",
    "\n",
    "3) Clean data thoroughly by removing:\n",
    "\n",
    "4) Links, special characters, and stop words.\n",
    "\n",
    "5) Non-stock-related posts using keywords like \"stock,\" \"market,\" or ticker symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb54e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "data[\"cleaned_title\"] = data[\"title\"].apply(clean_text)\n",
    "data[\"cleaned_body\"] = data[\"body\"].apply(clean_text)\n",
    "\n",
    "# Sentiment Analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data[\"title_sentiment\"] = data[\"cleaned_title\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "data[\"body_sentiment\"] = data[\"cleaned_body\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "# TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "tfidf_matrix = vectorizer.fit_transform(data[\"cleaned_title\"] + \" \" + data[\"cleaned_body\"])\n",
    "\n",
    "# Save TF-IDF as a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "data = pd.concat([data.reset_index(drop=True), tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57bf96",
   "metadata": {},
   "source": [
    "### Step 3: Feature Engineering\n",
    "\n",
    "1) Include post popularity (score, num_comments) as features.\n",
    "\n",
    "2) Extract mentions of stock tickers to track relevance to specific stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "291ef53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature: Mentions of \"stock\" or ticker symbols\n",
    "data[\"mentions_stock\"] = data[\"cleaned_body\"].apply(lambda x: 1 if \"stock\" in x or \"$\" in x else 0)\n",
    "\n",
    "# Final feature set\n",
    "features = data[[\"title_sentiment\", \"body_sentiment\", \"score\", \"num_comments\", \"mentions_stock\"] + list(tfidf_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f2be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate stock price movement labels (1 for up, 0 for down)\n",
    "import numpy as np\n",
    "data[\"stock_movement\"] = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# Select features and labels\n",
    "features = data[[\"title_sentiment\", \"body_sentiment\", \"score\", \"num_comments\"]]\n",
    "labels = data[\"stock_movement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b8c8a",
   "metadata": {},
   "source": [
    "### Step 4: Building and Improving the Prediction Model\n",
    "Model Selection:\n",
    "\n",
    "--> Start with a simple model (e.g., Logistic Regression) and move to advanced models like:\n",
    "\n",
    "2) Random Forest\n",
    "\n",
    "3) XGBoost\n",
    "\n",
    "4) LSTM or Transformer models (for deep learning on text data).\n",
    "\n",
    "5) Use GridSearchCV for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52053228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50        21\n",
      "           1       0.52      0.68      0.59        19\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.56      0.56      0.55        40\n",
      "weighted avg       0.56      0.55      0.54        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define labels (example: price up = 1, price down = 0)\n",
    "labels = data[\"stock_movement\"]  # Assuming stock_movement is defined\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model and evaluation\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789deb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
